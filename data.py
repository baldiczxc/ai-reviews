import pandas as pd
import re
from datasets import load_dataset
from tqdm import tqdm
import numpy as np
import os

# ===================== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ =====================
OUTPUT_FILE = "filtered_wb_feedbacks.parquet"
CHUNK_SIZE = 150000  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 50–∫ —Å—Ç—Ä–æ–∫ –∑–∞ —Ä–∞–∑
TEMP_DIR = "temp_chunks"  # –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤

# –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
os.makedirs(TEMP_DIR, exist_ok=True)

# ===================== –§—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö =====================
def remove_personal_data(text):
    """–£–¥–∞–ª—è–µ—Ç –∏–º–µ–Ω–∞, —Ñ–∞–º–∏–ª–∏–∏ –∏ –¥—Ä—É–≥–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    if not isinstance(text, str):
        return text
    
    # –£–¥–∞–ª—è–µ–º –∏–º–µ–Ω–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ "–ò–º—è, –¥–æ–±—Ä—ã–π –¥–µ–Ω—å"
    text = re.sub(r'^[–ê-–Ø–Å][–∞-—è—ë]+[,!.]?\s+(–¥–æ–±—Ä—ã–π|–ø—Ä–∏–≤–µ—Ç|–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ|–∏–∑–≤–∏–Ω–∏—Ç–µ)', '', text, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞—â–µ–Ω–∏—è —Å –∏–º–µ–Ω–∞–º–∏
    text = re.sub(r'(—É–≤–∞–∂–∞–µ–º—ã[–µ–π]|–¥–æ—Ä–æ–≥[–æ–π–∞—è])\s+[–ê-–Ø–Å][–∞-—è—ë]+', '', text, flags=re.IGNORECASE)
    
    # –£–¥–∞–ª—è–µ–º email –∞–¥—Ä–µ—Å–∞
    text = re.sub(r'\S+@\S+', '[EMAIL]', text)
    
    # –£–¥–∞–ª—è–µ–º –Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤
    text = re.sub(r'(\+7|8)[\s\-\(\)]*\d{3}[\s\-\(\)]*\d{3}[\s\-\(\)]*\d{2}[\s\-\(\)]*\d{2}', '[PHONE]', text)
    
    return text.strip()

# ===================== –£–º–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è —á–∞–Ω–∫–∞ =====================
def process_chunk(chunk_data):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ –¥–∞–Ω–Ω—ã—Ö"""
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è —Ä–µ–∫–ª–∞–º—ã
    ad_phrases = [
        '–∂–¥–µ–º –≤–∞—Å —Å–Ω–æ–≤–∞', '–∂–¥—ë–º –≤–∞—Å —Å–Ω–æ–≤–∞', '–∂–¥–µ–º –∑–∞ –ø–æ–∫—É–ø–∫–∞–º–∏', '–∂–¥—ë–º –∑–∞ –ø–æ–∫—É–ø–∫–∞–º–∏',
        '–ø—Ä–∏—è—Ç–Ω—ã—Ö –ø–æ–∫—É–ø–æ–∫', '–Ω–∞–¥–µ–µ–º—Å—è —É–≤–∏–¥–µ—Ç—å –≤–∞—Å —Å–Ω–æ–≤–∞', '—Å –Ω–∞–¥–µ–∂–¥–æ–π –≤–∏–¥–µ—Ç—å', 
        '–±—É–¥–µ–º —Ä–∞–¥—ã –≤–∏–¥–µ—Ç—å', '–∂–¥–µ–º –≤ –Ω–∞—à–µ–º –º–∞–≥–∞–∑–∏–Ω–µ', '–∂–¥—ë–º –≤ –Ω–∞—à–µ–º –º–∞–≥–∞–∑–∏–Ω–µ',
        '–ø–æ–∫—É–ø–∞–π', '–∑–∞–∫–∞–∑—ã–≤–∞–π', '–ø—Ä–∏–æ–±—Ä–µ—Ç–∞–π', '—Å–∫–∏–¥–∫', '–≤—ã–≥–æ–¥–Ω', '–∞–∫—Ü–∏', 
        '—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', '–ª—É—á—à–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ', '—É—Å–ø–µ–π—Ç–µ –∑–∞–∫–∞–∑–∞—Ç—å', 
        '—É—Å–ø–µ–π—Ç–µ –∫—É–ø–∏—Ç—å', '–ø–æ–¥–∞—Ä–æ–∫ –∫–∞–∂–¥–æ–º—É', '–∫–∞—Ç–∞–ª–æ–≥', '–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç',
        '—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å', '—Å–æ–≤–µ—Ç—É–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å', '–æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞', 
        '–ø–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ', '–Ω–∞—à–∏ —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∏', '–Ω–∞—à–∏ –ø–æ–¥—Ä—É–≥–∏', '–º—ã —É–≤–µ—Ä–µ–Ω—ã —á—Ç–æ',
        '—É–≤–µ—Ä–µ–Ω—ã —á—Ç–æ –≤—ã —Å–º–æ–∂–µ—Ç–µ –ø–æ–¥–æ–±—Ä–∞—Ç—å'
    ]

    results = []
    cleaned_texts = []
    cleaned_answers = []
    filtered_data = []

    for item in chunk_data:
        original_text = str(item.get('text', ''))
        original_answer = str(item.get('answer', ''))

        # –û—á–∏—â–∞–µ–º –æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        cleaned_text = remove_personal_data(original_text)
        cleaned_answer = remove_personal_data(original_answer)

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ
        if len(cleaned_text) < 10 or len(cleaned_answer) < 10:
            continue

        answer_lower = cleaned_answer.lower()

        # 1. –ñ–ï–°–¢–ö–ò–ï –ö–†–ò–¢–ï–†–ò–ò –û–¢–°–ï–ò–í–ê–ù–ò–Ø
        is_advertisement = (
            re.search(r'–∞—Ä—Ç\.\s*\d+|–∞—Ä—Ç–∏–∫—É–ª\s*\d+', answer_lower) or
            re.search(r'—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º.*\d+|–ø—Ä–µ–¥–ª–∞–≥–∞–µ–º.*\d+', answer_lower) or
            re.search(r'[#‚Ññ]\d+', answer_lower) or
            any(phrase in answer_lower for phrase in ad_phrases)
        )
        
        is_template = (
            len(cleaned_answer) < 25 or
            (answer_lower.startswith(('—Å–ø–∞—Å–∏–±–æ', '–±–ª–∞–≥–æ–¥–∞—Ä', '–∏–∑–≤–∏–Ω')) and len(cleaned_answer) < 50) or
            any(phrase in answer_lower for phrase in [
                '—Å–ø–∞—Å–∏–±–æ –∑–∞ –æ—Ç–∑—ã–≤', '–±–ª–∞–≥–æ–¥–∞—Ä–∏–º –∑–∞ –æ—Ç–∑—ã–≤', '–æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É',
                '–Ω–∞–ø–∏—à–∏—Ç–µ –Ω–∞–º', '–ø–æ–∑–≤–æ–Ω–∏—Ç–µ –Ω–∞–º', '–∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω',
                '–≤–∞—à –æ—Ç–∑—ã–≤ –æ—á–µ–Ω—å –≤–∞–∂–µ–Ω', '–±—É–¥–µ–º —Ä–∞–¥—ã –ø–æ–º–æ—á—å', '–ø—Ä–∏–Ω–æ—Å–∏–º –∏–∑–≤–∏–Ω–µ–Ω–∏—è'
            ])
        )

        has_junk = (
            re.search(r'http|www|\.ru|\.com', cleaned_answer) or
            re.search(r'[0-9]{10,}', cleaned_answer) or
            '[EMAIL]' in cleaned_answer or
            '[PHONE]' in cleaned_answer
        )

        # 2. –ö–†–ò–¢–ï–†–ò–ò –ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ì–û –û–¢–í–ï–¢–ê
        text_words = set(word for word in cleaned_text.lower().split() if len(word) > 4)
        answer_words = set(cleaned_answer.lower().split())
        common_words = text_words & answer_words
        has_relevance = len(common_words) > 0

        has_quality = (
            len(cleaned_answer.split()) > 8 and
            any(char in cleaned_answer for char in '.!?') and
            any(word in answer_lower for word in [
                '–∫–∞—á–µ—Å—Ç–≤', '–¥–æ—Å—Ç–∞–≤–∫', '—Ä–∞–∑–º–µ—Ä', '—Ü–≤–µ—Ç', '–º–∞—Ç–µ—Ä–∏–∞–ª',
                '–≥–∞—Ä–∞–Ω—Ç–∏', '–æ–±–º–µ–Ω', '–≤–æ–∑–≤—Ä–∞—Ç', '—Ä–µ–∫–æ–º–µ–Ω–¥', '—Å–æ–≤–µ—Ç—É',
                '–ø—Ä–æ–±–ª–µ–º', '—Ä–µ—à–µ–Ω', '–∏–∑–º–µ–Ω–µ–Ω', '—É–ª—É—á—à–µ–Ω', '–∏—Å–ø—Ä–∞–≤–ª–µ–Ω'
            ]) and
            not any(ad_phrase in answer_lower for ad_phrase in ['–∫—É–ø–∏—Ç', '–∑–∞–∫–∞–∑', '–ø–æ–∫—É–ø–∫', '–º–∞–≥–∞–∑–∏–Ω', '–∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç'])
        )

        # 3. –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï
        should_keep = (
            has_relevance and
            has_quality and
            not is_template and
            not has_junk and
            not is_advertisement
        )

        if should_keep:
            filtered_item = item.copy()
            filtered_item['text_cleaned'] = cleaned_text
            filtered_item['answer_cleaned'] = cleaned_answer
            filtered_data.append(filtered_item)

    return filtered_data

# ===================== –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è =====================
def main():
    print("–ó–∞–≥—Ä—É–∂–∞–µ–º wb-feedbacks...")
    
    try:
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º num_proc –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        dataset = load_dataset("nyuuzyou/wb-feedbacks", split="train", num_proc=4)
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} —Å—Ç—Ä–æ–∫")
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
        print("–ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥...")
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ - —Å–∫–∞—á–∏–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ
        try:
            # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_dataset("nyuuzyou/wb-feedbacks", split="train", cache_dir="./dataset_cache")
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset)} —Å—Ç—Ä–æ–∫")
        except Exception as e2:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç: {e2}")
            return

    total_processed = 0
    total_filtered = 0
    chunk_files = []

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    num_chunks = (len(dataset) + CHUNK_SIZE - 1) // CHUNK_SIZE
    
    for chunk_idx in tqdm(range(num_chunks), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤"):
        start_idx = chunk_idx * CHUNK_SIZE
        end_idx = min((chunk_idx + 1) * CHUNK_SIZE, len(dataset))
        
        chunk_data = dataset.select(range(start_idx, end_idx))
        total_processed += len(chunk_data)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —á–∞–Ω–∫
        filtered_data = process_chunk(chunk_data)
        total_filtered += len(filtered_data)
        
        if len(filtered_data) > 0:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_file = f"{TEMP_DIR}/chunk_{chunk_idx}.parquet"
            filtered_df = pd.DataFrame(filtered_data)
            filtered_df.to_parquet(temp_file, index=False)
            chunk_files.append(temp_file)
        
        print(f"\n–ß–∞–Ω–∫ {chunk_idx + 1}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(chunk_data)}, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered_data)}")
        
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞–Ω–∫–∏
    if chunk_files:
        print("–û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã...")
        all_chunks = []
        for file in chunk_files:
            chunk_df = pd.read_parquet(file)
            all_chunks.append(chunk_df)
        
        final_df = pd.concat(all_chunks, ignore_index=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        final_df['formatted_text'] = "–û—Ç–∑—ã–≤: " + final_df['text_cleaned'] + "\n–û—Ç–≤–µ—Ç: " + final_df['answer_cleaned']
        final_df.to_parquet(OUTPUT_FILE, index=False)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_processed} —Å—Ç—Ä–æ–∫")
        print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {total_filtered} —Å—Ç—Ä–æ–∫")
        print(f"–ü—Ä–æ—Ü–µ–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö: {total_filtered/total_processed*100:.1f}%")
        print(f"üö´ –û—Ç—Å–µ—è–Ω–æ: {total_processed - total_filtered} —Å—Ç—Ä–æ–∫")
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
        
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        for file in chunk_files:
            os.remove(file)
        os.rmdir(TEMP_DIR)
    
    else:
        print("‚ùå –ù–µ –æ—Å—Ç–∞–ª–æ—Å—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

if __name__ == "__main__":
    main()